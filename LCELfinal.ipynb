{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b902a29-5c27-4553-a75a-68fcb836c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d9fa9e-18a9-4b15-8c80-af0c07380b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8d72aa-e94d-4fe0-9cc7-2c3ff8127b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI.\n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it doesn't know.\n",
    "\n",
    "Current conversation:\n",
    "{message_log}\n",
    "\n",
    "Human:\n",
    "{question}\n",
    "\n",
    "AI:\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template = TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19714610-5380-453a-b3ee-9e28da9db5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\anaconda3\\envs\\langchain_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3519: UserWarning: Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(max_tokens = 100,\n",
    "                  model_name = 'gpt-4.1-mini',\n",
    "                  model_kwargs = {'seed': 365},\n",
    "                  temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e45bf1f-437c-4c1d-bf2b-4531116c7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory = ConversationSummaryMemory(llm = ChatOpenAI(),\n",
    "                                       memory_key = \"message_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4392b4cd-7b15-48fb-b1d9-87abc15c353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best book to read if i want to learn about economy?\"\n",
    "#question = \"Can you give me some intresting book to read that is not too popular?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72c923aa-318c-40f0-9a77-8413bd84dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_output = RunnablePassthrough.assign(message_log = RunnableLambda(chat_memory.load_memory_variables) | itemgetter('message_log')).invoke({'question': question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f52837d0-4e32-410f-9d32-2cc1e3cf59a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value_output = prompt_template.invoke(dictionary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b01b5c9a-f08e-4912-8fa6-0865a11a6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message_output = chat.invoke(prompt_value_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5111fe8-8220-49aa-b6fe-c337d4c379c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = StrOutputParser().invoke(ai_message_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8378e52-5f14-4acc-90a8-f77df8ba6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory.save_context(inputs = {'input': question},\n",
    "                        outputs = {'output': response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc50a89-47db-4a8d-b41a-0515de9aa055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': 'The human asks for an interesting book recommendation that is not too popular. The AI suggests \"Stoner\" by John Williams, a novel that explores themes of perseverance, love, and the passage of time with subtle depth and emotional complexity. When asked for a book recommendation on economics, the AI suggests \"Economics in One Lesson\" by Henry Hazlitt as a clear and accessible introduction to fundamental economic principles.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d6a3ab4-ca2c-4291-8bf2-ae8f57846e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here are a few more vacation spot suggestions, each with unique experiences depending on what youâ€™re looking for:\\n\\n1. **Kyoto, Japan**  \\n   If you love history, culture, and stunning temples, Kyoto is a fantastic choice. You can explore over 1,600 Buddhist temples, traditional tea houses, and beautiful gardens. The city is especially magical during cherry blossom season in spring or the vibrant autumn foliage.\\n\\n2. **Amalfi Coast, Italy**  \\n   For breathtaking'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1 = (\n",
    "    RunnablePassthrough.assign(\n",
    "        message_log = RunnableLambda(chat_memory.load_memory_variables)\n",
    "         | itemgetter('message_log'))\n",
    "    | prompt_template\n",
    "    | chat\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What else can you suggest me?\"\n",
    "\n",
    "response = chain1.invoke({'question':question})\n",
    "\n",
    "chat_memory.save_context(inputs = {'input':question},\n",
    "                         outputs = {'output': response})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a776d72d-80bd-4937-8cfc-df62defcc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def memory_chain(question) :\n",
    "    chain1 = (\n",
    "        RunnablePassthrough.assign(\n",
    "            message_log = RunnableLambda(chat_memory.load_memory_variables)\n",
    "             | itemgetter('message_log'))\n",
    "        | prompt_template\n",
    "        | chat\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    chain1.get_graph().print_ascii()\n",
    "    \n",
    "    response = chain1.invoke({'question':question})\n",
    "    \n",
    "    chat_memory.save_context(inputs = {'input':question},\n",
    "                             outputs = {'output': response})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26ba7571-159c-477d-98e5-50cc4efe17b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +----------------------------+          \n",
      "              | Parallel<message_log>Input |          \n",
      "              +----------------------------+          \n",
      "                    **             ***                \n",
      "                 ***                  ***             \n",
      "               **                        **           \n",
      "+-----------------------+                  **         \n",
      "| load_memory_variables |                   *         \n",
      "+-----------------------+                   *         \n",
      "            *                               *         \n",
      "            *                               *         \n",
      "            *                               *         \n",
      "        +--------+                   +-------------+  \n",
      "        | Lambda |                   | Passthrough |  \n",
      "        +--------+**                 +-------------+  \n",
      "                    **              **                \n",
      "                      ***        ***                  \n",
      "                         **    **                     \n",
      "             +-----------------------------+          \n",
      "             | Parallel<message_log>Output |          \n",
      "             +-----------------------------+          \n",
      "                            *                         \n",
      "                            *                         \n",
      "                            *                         \n",
      "                    +----------------+                \n",
      "                    | PromptTemplate |                \n",
      "                    +----------------+                \n",
      "                            *                         \n",
      "                            *                         \n",
      "                            *                         \n",
      "                      +------------+                  \n",
      "                      | ChatOpenAI |                  \n",
      "                      +------------+                  \n",
      "                            *                         \n",
      "                            *                         \n",
      "                            *                         \n",
      "                   +-----------------+                \n",
      "                   | StrOutputParser |                \n",
      "                   +-----------------+                \n",
      "                            *                         \n",
      "                            *                         \n",
      "                            *                         \n",
      "                +-----------------------+             \n",
      "                | StrOutputParserOutput |             \n",
      "                +-----------------------+             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Absolutely! Here are a few more options you might consider within the 5,000 euros budget, typically looking at used models around 8 to 15 years old:\\n\\n1. **Renault Clio (2005-2010)** â€“ A popular supermini in Europe, known for its comfortable ride and decent fuel economy. Parts and servicing are generally affordable, making it a practical choice.\\n\\n2. **Peugeot 207 (2006-2012)** â€“ Stylish and comfortable, the '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_chain.invoke(\"Do you have some other cars to suggest me?\") #Change question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0f0e6-b633-426f-840a-e7a0e711fe23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
